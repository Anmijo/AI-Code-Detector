{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **How to Run Code and Setup Environment**\n",
        "\n",
        "Setting up process is pretty straightforward.\n",
        "\n",
        "Make sure the dataV3.csv is save in the correct directory. To change path, change it in the \"Setup DataSet and API\" section.\n",
        "\n",
        "Make sure to change the OpenAI API key. For instructors, please Contact me to give you correct key.\n",
        "\n",
        "Run all the code sections.\n",
        "\n",
        "To detect a new code sample. Put that code sample(without comments blank lines) in the \"Detect with New Code\" section and run that segment.\n"
      ],
      "metadata": {
        "id": "18oDtwfY-4LB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Libraries**"
      ],
      "metadata": {
        "id": "gB0RNsPOwaBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X18uG0oUxxM1",
        "outputId": "4ef7ff4d-5381-496d-bf27-b096225bbf25"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.28) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "EZH2UDtoKupR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "import openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup Dataset and API**"
      ],
      "metadata": {
        "id": "KKMzkIdZyFAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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' # remember to put correct api key\n",
        "\n",
        "data = pd.read_csv('/content/dataV3.csv') # remember to change path\n",
        "label_mapping = {\"H\": 0, \"A\": 1}\n",
        "data['label(H/A)'] = data['label(H/A)'].map(label_mapping)\n",
        "\n",
        "train_texts, eval_texts, train_labels, eval_labels = train_test_split(\n",
        "    data['code'], data['label(H/A)'], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "ug5TuX3Fyezo"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Fine-tune Code-Bert model**"
      ],
      "metadata": {
        "id": "fBzgBMSWywIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained CodeBERT model to tokenize code\n",
        "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
        "\n",
        "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
        "eval_encodings = tokenizer(list(eval_texts), truncation=True, padding=True, max_length=512)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQTtpcrMy9Ix",
        "outputId": "09d8f7c5-0224-4ad7-d7bf-2b590f8b8692"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "p3fReH0BzA37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class for code text and labels\n",
        "class CodeDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Create training and evaluation datasets\n",
        "train_dataset = CodeDataset(train_encodings, list(train_labels))\n",
        "eval_dataset = CodeDataset(eval_encodings, list(eval_labels))\n",
        "\n",
        "# Initialize model for sequence classification and fine-tuning\n",
        "model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)\n",
        "\n",
        "# Set training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=7,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=200,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,\n",
        ")\n",
        "\n",
        "# Create trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "eHTnNnbfzE0n",
        "outputId": "3ff79945-2922-4dcf-c03f-925a79100653"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [140/140 01:18, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.735120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.656988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.557230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.491943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.451690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.457009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.364558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=140, training_loss=0.6437094007219587, metrics={'train_runtime': 78.7256, 'train_samples_per_second': 7.113, 'train_steps_per_second': 1.778, 'total_flos': 147342191001600.0, 'train_loss': 0.6437094007219587, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Calculating Perplexity, Perturbation, Scoring, and Classifying**"
      ],
      "metadata": {
        "id": "crj3gXmLzLDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filling_model = RobertaForCausalLM.from_pretrained('microsoft/codebert-base')\n",
        "\n",
        "# Function to calculate perplexity\n",
        "def calculate_perplexity(code):\n",
        "    # Use GPT 3.5 Turbo model to calculate perplexity\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": code}],\n",
        "            max_tokens=1,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        response_text = response.choices[0].message.content\n",
        "        perplexity = len(code.split()) / max(1, len(response_text.split()))\n",
        "        return perplexity\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating perplexity: {e}\")\n",
        "        return float('inf')\n",
        "\n",
        "# Function to apply targeted masking and perturb\n",
        "def apply_targeted_masking(code, line_perplexities, mask_ratio=0.2):\n",
        "    inputs = tokenizer(code, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    num_tokens = inputs.input_ids.size(1)\n",
        "    num_tokens_to_mask = int(mask_ratio * num_tokens)\n",
        "\n",
        "    # Mask/identify areas with high-perplexity\n",
        "    high_ppl_lines = [i for i, ppl in enumerate(line_perplexities) if ppl > np.mean(line_perplexities)]\n",
        "    mask_indices = torch.randperm(num_tokens)[:num_tokens_to_mask]\n",
        "    masked_inputs = inputs.input_ids.clone()\n",
        "    masked_inputs[0, mask_indices] = tokenizer.mask_token_id\n",
        "\n",
        "    # Generate the filled/perturbed code with CodeBERT\n",
        "    with torch.no_grad():\n",
        "        outputs = mask_filling_model.generate(masked_inputs, max_new_tokens=50)\n",
        "    perturbed_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return perturbed_code\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Calculate all metrics for score calculation\n",
        "def compute_metrics(code):\n",
        "    # Calculate overall perplexity\n",
        "    original_perplexity = calculate_perplexity(code)\n",
        "\n",
        "    # Calculate line-by-line perplexities\n",
        "    lines = code.split('\\n')\n",
        "    line_perplexities = [calculate_perplexity(line) for line in lines]\n",
        "\n",
        "    # Calculate standard deviation of line perplexities\n",
        "    std_dev_perplexity = np.std(line_perplexities)\n",
        "\n",
        "    # Calculate burstiness based on the frequence of words in the code sample\n",
        "    tokens = code.split()  # Split into tokens by seperating by whitespaces\n",
        "    token_counts = Counter(tokens) # Count how many times each word has been repeated\n",
        "    burstiness = np.mean([count for token, count in token_counts.items() if count > 1]) # Calculate mean of repeated words\n",
        "\n",
        "    return original_perplexity, std_dev_perplexity, burstiness\n",
        "\n",
        "# Calculate the score for original and perturbed code\n",
        "def calculate_score(code, perturbed_code):\n",
        "    original_ppl, std_dev_ppl, burstiness = compute_metrics(code)\n",
        "    perturbed_ppl, std_perturbed, burstiness_perturbed = compute_metrics(perturbed_code)\n",
        "\n",
        "    # Formula to calclate score\n",
        "    score = 0.6 * original_ppl + 0.2 * std_dev_ppl + 0.2 * burstiness\n",
        "    perturbed_score = 0.6 * perturbed_ppl + 0.2 * std_perturbed + 0.2 * burstiness_perturbed\n",
        "\n",
        "    # Produce Output\n",
        "    print(f\"Original PPl: {original_ppl}\")\n",
        "    print(f\"Std dev PPl: {std_dev_ppl}\")\n",
        "    print(f\"Burstiness: {burstiness}\")\n",
        "    print(\"\")\n",
        "    print(f\"Perturbed PPl: {perturbed_ppl}\")\n",
        "    print(f\"Std dev PPL for Perturbed: {std_perturbed}\")\n",
        "    print(f\"Burstiness for Perturbed: {burstiness_perturbed}\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    return score, perturbed_score\n",
        "\n",
        "# Classification based on score\n",
        "def classify_code(code, threshold=0.8): # Change threshhold\n",
        "    line_perplexities = [calculate_perplexity(line) for line in code.split('\\n')]\n",
        "    perturbed_code = apply_targeted_masking(code, line_perplexities)\n",
        "\n",
        "    original_score, perturbed_score = calculate_score(code, perturbed_code)\n",
        "    print(f\"Original Score: {original_score}, Perturbed Score: {perturbed_score}\")\n",
        "\n",
        "    if original_score > perturbed_score * threshold:\n",
        "        print(\"The code is likely AI-generated.\")\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"The code is likely human-written.\")\n",
        "        return 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef_wcsJDzX28",
        "outputId": "fbbe530e-8033-47e6-cbdf-d9566fe0b516"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Detect with new code**"
      ],
      "metadata": {
        "id": "y3byWzhq7eiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_code = \"\"\"\n",
        "def solve():\n",
        "    import sys\n",
        "    input = sys.stdin.read\n",
        "    data = input().split()\n",
        "    N = int(data[0])\n",
        "    a = list(map(int, data[1:]))\n",
        "    operations = []\n",
        "    if a[0] > a[-1]:\n",
        "        max_idx = a.index(max(a))\n",
        "        operations += [(max_idx + 1, i + 1) for i in range(N) if a[i] < a[max_idx]]\n",
        "        operations += [(i, i + 1) for i in range(1, N)]\n",
        "    else:\n",
        "        min_idx = a.index(min(a))\n",
        "        operations += [(min_idx + 1, i + 1) for i in range(N) if a[i] > a[min_idx]]\n",
        "        operations += [(i, i - 1) for i in range(N, 1, -1)]\n",
        "    print(len(operations))\n",
        "    for x, y in operations:\n",
        "        print(x, y)\n",
        "\"\"\"\n",
        "\n",
        "classification = classify_code(new_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhqXCIJc7rbR",
        "outputId": "2d41f0a9-1a14-4a77-d146-19df6d180128"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original PPl: 94.0\n",
            "Std dev PPl: 4.796873982084583\n",
            "Burstiness: 4.071428571428571\n",
            "\n",
            "Perturbed PPl: 101.0\n",
            "Std dev PPL for Perturbed: 5.860020797744664\n",
            "Burstiness for Perturbed: 3.6\n",
            "\n",
            "\n",
            "Original Score: 58.17366051070263, Perturbed Score: 62.49200415954893\n",
            "The code is likely AI-generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "v-efoqsV5Z7J"
      },
      "execution_count": 47,
      "outputs": []
    }
  ]
}